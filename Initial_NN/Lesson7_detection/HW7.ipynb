{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e682d4",
   "metadata": {},
   "source": [
    "### EfficientNet\n",
    "\n",
    "Авторы научной работы изучают скейлинг (масштабирование) моделей и балансирование между собой глубины и ширины (количества каналов) сети, а также разрешение изображений в сетке. Предлагают новый метод скейлинга, который равномерно скейлит глубину/ширину/разрешение. Показывают его эффективность на MobileNet и ResNet.\n",
    "\n",
    "\n",
    "Также используют Neural Architecture Search для создания новой сетки и скейлят её, тем самым получая класс новых моделей – EfficientNets. Они лучше и намного экономнее предыдущих сеток. На ImageNet EfficientNet-B7 достигает state-of-the-art 84.4% top-1 и 97.1% top-5 accuracy, будучи при этом в 8.4 раза меньше и в 6.1 раз быстрее на инференсе, чем текущая лучшая по точности ConvNet. Хорошо трансферится на другие датасеты – получили SOTA на 5 из 8 наиболее популярных датасетов.\n",
    "\n",
    "\n",
    "Compound model scaling\n",
    "Скейлинг – это когда фиксируются производимые внутри сетки операции и меняются лишь глубина (количество повторений одних и тех же модулей) d, ширина (количество каналов в свёртках) w и разрешение r. В пэйпере скейлинг формулируется как проблема оптимизации – хотим максимальную Accuracy(Net(d, w, r)) при том, что не вылезаем за границу по памяти и по FLOPS.\n",
    "\n",
    "\n",
    "Провели эксперименты и убедились в том, что действительно помогает также скейлить по глубине и разрешению, когда скейлим по ширине. При тех же FLOPS достигаем существенно лучшего результата на ImageNet (см картинку выше). Вообще это разумно, потому что кажется, что при увеличении разрешения изображения сети необходимо больше слоёв в глубину для увеличения рецептивного поля и больше каналов для того, чтобы ухватить все паттерны в изображении с более высоким разрешением.\n",
    "\n",
    "\n",
    "Суть compound scaling'а: берём compound coefficient phi, который с этим коэффициентом равномерно скейлит d, w и r: $d = \\alpha^\\phi, w = \\beta^\\phi, r = \\gamma^\\phi,$ где $\\alpha, \\beta, \\gamma$ – константы, полученные из небольшого грид сёрча по исходной сетке. $\\phi$ – коэффициент, характеризующий количество имеющихся вычислительных ресурсов.\n",
    "\n",
    "\n",
    "Efficient-Net\n",
    "Для создания сетки использовали Multi-objective neural architecture search, оптимизировали Accuracy и FLOPS с параметром, отвечающий за трейд-офф между ними. Такой поиск и дал EfficientNet-B0. Вкратце – Conv, за которой идут несколько MBConv, в конце Conv1х1, Pool, FC.\n",
    "\n",
    "\n",
    "Затем делаем скейлинг из двух шагов:\n",
    "\n",
    "\n",
    "Для начала фиксируем $\\phi = 1$, делаем грид сёрч для поиска $\\alpha, \\beta, \\gamma$.\n",
    "Скейлим сетку, используя формулы для d, w и r. Получили EffiientNet-B1. Аналогично, увеличивая $\\phi$, получаются EfficientNet-B2, … B7.\n",
    "\n",
    "Проводили скейлинг для разных ResNet и MobileNet, везде получили существенные улучшения на ImageNet, compound scaling давал значительный прирост по сравнению со скейлингом лишь по одному измерению. Также провели эксперименты с EfficientNet ещё на восьми популярных датасетах, везде получили SOTA или близкий к нему результат при существенно меньшем количестве параметров.\n",
    "\n",
    "\n",
    "Оригинал статьи - https://arxiv.org/abs/1905.11946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8aa97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
